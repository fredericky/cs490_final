{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_proj",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6528ec1abead4eb392b03d05fb7343ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_808a5d0d2e6c467497932bebf7aa7fc9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9fe1c05e96c741b0bd43a3defba90c1b",
              "IPY_MODEL_3f94ecab09a74e8188c91da73d117884"
            ]
          }
        },
        "808a5d0d2e6c467497932bebf7aa7fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fe1c05e96c741b0bd43a3defba90c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f3c41be0e584c3b8854df3353694a1e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_189c26137e2544f88c993b634c1c2505"
          }
        },
        "3f94ecab09a74e8188c91da73d117884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f388b4e2f6dd431eb07fc908854580af",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 710kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_578a1539029a4c2aace84f11d2db0749"
          }
        },
        "5f3c41be0e584c3b8854df3353694a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "189c26137e2544f88c993b634c1c2505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f388b4e2f6dd431eb07fc908854580af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "578a1539029a4c2aace84f11d2db0749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "639dbba995184392b24992b1dc8ddafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_947800e3302d42c6a579a42f243b6118",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a53ae0ce7ac4f3aa321692046850a01",
              "IPY_MODEL_de1c0dbd21a443e8bac4ccb828adbf9e"
            ]
          }
        },
        "947800e3302d42c6a579a42f243b6118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a53ae0ce7ac4f3aa321692046850a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fd419cbb74f848eaa39664915040ded9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f39e8d6a9ea4af3a98c9629eb4418fc"
          }
        },
        "de1c0dbd21a443e8bac4ccb828adbf9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a134969ccef540f4971c13343537953c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.66kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1951bc765630463693f9f714e8bb18d6"
          }
        },
        "fd419cbb74f848eaa39664915040ded9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f39e8d6a9ea4af3a98c9629eb4418fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a134969ccef540f4971c13343537953c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1951bc765630463693f9f714e8bb18d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b7100ab06c84050b2ee71bae86c946e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a51138dbe9b946878ac7618927c1cc22",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5612503f2f4346abb1b45a56f43b347b",
              "IPY_MODEL_0a19a627dfae406aafc34c6c1c1826e3"
            ]
          }
        },
        "a51138dbe9b946878ac7618927c1cc22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5612503f2f4346abb1b45a56f43b347b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02afd445421a4e7d8a43e68156033114",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eb6612392094516a77bd590ca75d659"
          }
        },
        "0a19a627dfae406aafc34c6c1c1826e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22aa8291fb9744c5a126464bd949e74f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:38&lt;00:00, 11.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_523fa73e0d5742a181ec48a280ea28b5"
          }
        },
        "02afd445421a4e7d8a43e68156033114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eb6612392094516a77bd590ca75d659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22aa8291fb9744c5a126464bd949e74f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "523fa73e0d5742a181ec48a280ea28b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA6VHVA8aAvU"
      },
      "source": [
        "**Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO3-n3CNR5Ib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ccd371-10c2-4e1b-84b9-c37def4dd981"
      },
      "source": [
        "!nvidia-smi\n",
        "!pip install -q -U watermark\n",
        "!pip install -q transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec 16 00:32:29 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 31.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 48.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BST-o3jyauZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4362b6c-fd61-4017-dc52-564e2eaeda8a"
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers,seaborn,matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.18.5\n",
            "pandas 1.1.5\n",
            "torch 1.7.0+cu101\n",
            "transformers 4.0.1\n",
            "seaborn 0.11.0\n",
            "matplotlib 3.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sQVK5BQaEj_"
      },
      "source": [
        "**Upload training/test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMaX0KZjUBku",
        "outputId": "0a81cebe-6508-4174-918b-c6607cf7b244"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqx8FekhU8Ae"
      },
      "source": [
        "import os\n",
        "BASE_PATH = '/gdrive/My Drive/colab_files/final_proj/'\n",
        "if not os.path.exists(BASE_PATH):\n",
        "  os.makedirs(BASE_PATH)\n",
        "\n",
        "os.chdir(BASE_PATH)\n",
        "\n",
        "for file in ['train.csv', 'test.csv']:\n",
        "  if not os.path.exists(os.path.join(BASE_PATH, file)):\n",
        "    print(f'please upload the file {file}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxJ8X_N7jFqc"
      },
      "source": [
        "**Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB0CHPsTgexw"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, get_linear_schedule_with_warmup, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG1STf2PjM31"
      },
      "source": [
        "**Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7sq-anVraax3",
        "outputId": "b7c5d16e-d949-45ba-c40d-445d6d2a004e"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "df.info()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   textID         27481 non-null  object\n",
            " 1   text           27480 non-null  object\n",
            " 2   selected_text  27480 non-null  object\n",
            " 3   sentiment      27481 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 858.9+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "CbpBoeW7gKgc",
        "outputId": "1bfa519b-a590-4f8e-9236-c3832b19308f"
      },
      "source": [
        "sns.countplot(df.sentiment)\n",
        "plt.xlabel('sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'sentiment')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1X3e9ajqIg"
      },
      "source": [
        "**Figure out sequence length**\n",
        "\n",
        "As BERT works with fixed-length sequenence, need to figure out this param"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416,
          "referenced_widgets": [
            "6528ec1abead4eb392b03d05fb7343ee",
            "808a5d0d2e6c467497932bebf7aa7fc9",
            "9fe1c05e96c741b0bd43a3defba90c1b",
            "3f94ecab09a74e8188c91da73d117884",
            "5f3c41be0e584c3b8854df3353694a1e",
            "189c26137e2544f88c993b634c1c2505",
            "f388b4e2f6dd431eb07fc908854580af",
            "578a1539029a4c2aace84f11d2db0749"
          ]
        },
        "id": "xeypC6Hhj6Gp",
        "outputId": "a4f41b41-e89f-46e7-f0b9-de4ad362b708"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "text_lens = []\n",
        "bad_text = 0\n",
        "valid_texts = []\n",
        "for text in df.text:\n",
        "  try:\n",
        "    tokens = tokenizer.encode(str(text), max_length=512)\n",
        "    text_lens.append(len(tokens))\n",
        "    valid_texts.append(text)\n",
        "  except:\n",
        "    print(text)\n",
        "    bad_text += 1\n",
        "\n",
        "print(f'{bad_text} bad texts')\n",
        "\n",
        "sns.distplot(text_lens)\n",
        "plt.xlim([0, 256]);\n",
        "plt.xlabel('Text length');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6528ec1abead4eb392b03d05fb7343ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0 bad texts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrkaVJ6pp2l"
      },
      "source": [
        "from the above, max_length with 150 looks good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G97_4jkGqiiE"
      },
      "source": [
        "**Prepare training/validation data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcCWA2ddqn6M",
        "outputId": "9be37a2f-c3d9-434e-8d62-646fc9e4294f"
      },
      "source": [
        "MAX_LENGTH = 150\n",
        "SENTIMENT = {\n",
        "    'positive' : 0,\n",
        "    'neutral': 1,\n",
        "    'negative': 2\n",
        "}\n",
        "\n",
        "class TweetsDataset(Dataset):\n",
        "  def __init__(self, tweets, labels):\n",
        "    self.tweets = tweets\n",
        "    self.labels = labels\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tweets)\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    tweet = self.tweets[i]\n",
        "    label = self.labels[i]\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        str(tweet), \n",
        "        add_special_tokens=True, \n",
        "        max_length=MAX_LENGTH, \n",
        "        return_token_type_ids=False, \n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return {\n",
        "        'tweet': tweet,\n",
        "        'label': torch.tensor(SENTIMENT[label], dtype=torch.long),\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "    }\n",
        "\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "ds_train = TweetsDataset(df_train.text.to_numpy(), df_train.sentiment.to_numpy())\n",
        "ds_val = TweetsDataset(df_val.text.to_numpy(), df_val.sentiment.to_numpy())\n",
        "ds_test = TweetsDataset(df_test.text.to_numpy(), df_test.sentiment.to_numpy())\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, num_workers=4)\n",
        "dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, num_workers=4)\n",
        "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=4)\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "print(len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21984\n",
            "5497\n",
            "3534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTfn5IpRvKn3"
      },
      "source": [
        "**Model & training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132,
          "referenced_widgets": [
            "639dbba995184392b24992b1dc8ddafe",
            "947800e3302d42c6a579a42f243b6118",
            "5a53ae0ce7ac4f3aa321692046850a01",
            "de1c0dbd21a443e8bac4ccb828adbf9e",
            "fd419cbb74f848eaa39664915040ded9",
            "0f39e8d6a9ea4af3a98c9629eb4418fc",
            "a134969ccef540f4971c13343537953c",
            "1951bc765630463693f9f714e8bb18d6",
            "7b7100ab06c84050b2ee71bae86c946e",
            "a51138dbe9b946878ac7618927c1cc22",
            "5612503f2f4346abb1b45a56f43b347b",
            "0a19a627dfae406aafc34c6c1c1826e3",
            "02afd445421a4e7d8a43e68156033114",
            "8eb6612392094516a77bd590ca75d659",
            "22aa8291fb9744c5a126464bd949e74f",
            "523fa73e0d5742a181ec48a280ea28b5"
          ]
        },
        "id": "j8w3TfOJv4wK",
        "outputId": "5b705f94-a1d2-492b-d086-dc2fa371638a"
      },
      "source": [
        "NUM_CLASSES = 3\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert_model = BertModel.from_pretrained('bert-base-cased')\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert_model.config.hidden_size, NUM_CLASSES)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    bert_outputs = self.bert_model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(bert_outputs.pooler_output)\n",
        "    return self.out(output)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "print('using device', device)\n",
        "\n",
        "model = SentimentClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "EPOCHS = 10\n",
        "total_steps = len(dl_train) * EPOCHS\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "def train(model):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  num_correct = 0\n",
        "  for d in dl_train:\n",
        "    input_ids = d['input_ids'].to(device)\n",
        "    attention_mask = d['attention_mask'].to(device)\n",
        "    label = d['label'].to(device)\n",
        "\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    _, prediction = torch.max(output, dim=1)\n",
        "    loss = loss_fn(output, label)\n",
        "    num_correct += torch.sum(prediction == label)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return num_correct.double() / len(df_train), np.mean(losses)\n",
        "\n",
        "def evaluate(model, dl, df):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  num_correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in dl:\n",
        "      input_ids = d['input_ids'].to(device)\n",
        "      attention_mask = d['attention_mask'].to(device)\n",
        "      label = d['label'].to(device)\n",
        "\n",
        "      output = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, prediction = torch.max(output, dim=1)\n",
        "      loss = loss_fn(output, label)\n",
        "      num_correct += torch.sum(prediction == label)\n",
        "      losses.append(loss.item())\n",
        "  return num_correct.double() / len(df), np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "639dbba995184392b24992b1dc8ddafe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b7100ab06c84050b2ee71bae86c946e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-seVnY3f3j7y",
        "outputId": "eeaf36ad-3a9a-4436-cf79-4875b319365e"
      },
      "source": [
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "  train_acc, train_loss = train(model)\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}\\n')\n",
        "\n",
        "  val_acc, val_loss = evaluate(model, dl_val, df_val)\n",
        "  print(f'Val loss {val_loss} accuracy {val_acc}\\n')\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model.bin')\n",
        "    best_accuracy = val_acc\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('History')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.6853332175958625 accuracy 0.7104712518195051\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 0.5682900819580915 accuracy 0.7700563943969438\n",
            "\n",
            "Epoch 2/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.45867468857815086 accuracy 0.8263737263464338\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 0.63601977588219 accuracy 0.7611424413316354\n",
            "\n",
            "Epoch 3/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.33158209399636046 accuracy 0.8844159388646289\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 0.7496250784024596 accuracy 0.7671457158450065\n",
            "\n",
            "Epoch 4/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.24776545977758804 accuracy 0.926401018922853\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 0.9736509345455615 accuracy 0.7647807895215573\n",
            "\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.18693198364834995 accuracy 0.9512372634643378\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.1983984652879582 accuracy 0.7653265417500456\n",
            "\n",
            "Epoch 6/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14573897955836823 accuracy 0.9652019650655023\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.3200521655380726 accuracy 0.7669637984355103\n",
            "\n",
            "Epoch 7/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11434614181857802 accuracy 0.9747998544395925\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.4255867470800334 accuracy 0.7647807895215573\n",
            "\n",
            "Epoch 8/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.08640725832998318 accuracy 0.9819414119359534\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.4944015391550682 accuracy 0.7698744769874477\n",
            "\n",
            "Epoch 9/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.06910085754147581 accuracy 0.9859898107714702\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.5213720954077787 accuracy 0.7675095506639986\n",
            "\n",
            "Epoch 10/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.05493503184382447 accuracy 0.9894923580786027\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss 1.5474589450405316 accuracy 0.7687829725304712\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypMkHzdz5MDS"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRtciK5a5NvM",
        "outputId": "d45fbf1d-0e2a-4f99-9c23-09fad6fda134"
      },
      "source": [
        "test_acc, _ = evaluate(model, dl_test, df_test)\n",
        "test_acc.item()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7744765138653085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}